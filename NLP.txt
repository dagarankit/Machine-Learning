first part of nlp is tokenization-

where a paragraph is divided or splitted in sentences.
or we can say text processing

we can say paragraphs as corps

import nltk
nltk.download()

paragraph -"""dofweejoijtotjotjreotjreoteto group of sentences nkjrenrenerne"""

 
nltk.sent_tokenize function is responsible for converting paragraps into sentences

sentences = nltk.sent_tokenize(paragraph)

and sentences would be lists

for example we can say there are 31 unique sentences in list in paragraph 

text pre processing works on each and every word

Now next process is to convert sentences into word to calculate how many words are present in graph


words = nltk.word_tokenize(paragraph)

nltk.word_tokenize responsible for converting sentences into words

every punctuation, ',' or any ! is taken as word and may be having repeated words


we are performing text pre processing 

Next text preprocessing is Stemming

Stemming: Stemming is a process of reducing infected or derived words to their 
word stem, base or root form.

like 

intelligence   }
intelligent    }      all are derived from intelligen (stemming word)   
intelligently  }

why do we need stemmimg

for ex if we need to perform negative and positive sentiment analysis
so words in diff form can not be helpful 
so we try to find out the root or base word

library used for stemming from nltk.stem import PorterStemmer
from nltk.corpus import stopwords 

here we are using stopwords to remove the unvaluable or repititive words given in paragraph 
like of , them ,our etc but also having other applications

For stemming first step is to import PorterStemmer library 
then creating object of that function say stemmer

stemmer = PorterStemmer()

we can see the stopwords of any language like english french etc 
for this we need to call a function that is 

stopwords.words('english')








TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:


TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).


IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:


IDF(t) = log_e(Total number of documents / Number of documents with term t in it).

See below for a simple example.


